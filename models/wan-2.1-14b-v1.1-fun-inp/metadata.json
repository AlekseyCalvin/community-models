{
  "name" : "Wan 2.1 14B v1.1 Fun InP",
  "version" : "wan_v2.1_14b",
  "modifier" : "inpainting",
  "autoencoder" : "wan_v2.1_video_vae_f16.ckpt",
  "prefix" : "",
  "default_scale" : 12,
  "hires_fix_scale" : 16,
  "file" : "wan_2.1_14b_v1.1_fun_inp_q8p.ckpt",
  "upcast_attention" : false,
  "text_encoder" : "umt5_xxl_encoder_q8p.ckpt",
  "clip_encoder" : "open_clip_xlm_roberta_large_vit_h14_f16.ckpt",
  "high_precision_autoencoder": false,
  "objective" : {
    "u" : {
      "condition_scale" : 1000
    }
  },
  "tea_cache_coefficients": [8.10705460e+03,  2.13393892e+03, -3.72934672e+02,  1.66203073e+01, -4.17769401e-02],
  "note": "[Wan2.1 14B v1.1 Fun InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-InP) is a state-of-the-art image-to-video model developed by Alibaba PAI. It can generate video clips of up to 4 seconds in length. The model is trained at multiple resolutions, supports up to 81 frames, with a recommended shift value of 5.0. Text Guidance should be somewhere between 3 to 7. Wan2.1 is trained with Flow Matching objective, Trailing samplers would give the best result.",
  "converted": {
    "wan_v2.1_video_vae_f16.ckpt": "4c518b128b3c1f2ea164aa46269d8875b4be3661d1fc0fba2709d03fa94e418b",
    "umt5_xxl_encoder_q8p.ckpt": "72ef62d22c09a3b764ac9e6fe0100f4029619fb3ff8ccec3432e509487b29831",
    "open_clip_xlm_roberta_large_vit_h14_f16.ckpt": "362c9940a36acce5a4e13b9167d5daebd005ac026443cd37b7955ac0acd72083",
    "wan_2.1_14b_v1.1_fun_inp_q6p_svd.ckpt": "7e6c71bf0d927a9b1587c3fba8e9633c31dcd3ef43a9405cbf70240e7a8fcb44",
    "wan_2.1_14b_v1.1_fun_inp_q8p.ckpt": "e6d98deb33b959fb006cdcb94b7b2d3d05999a88948451e5dce75250a4f028b6"
  }
}
